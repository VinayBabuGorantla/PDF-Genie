from langchain.chains import ConversationalRetrievalChain
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline
from langchain.memory import ConversationBufferMemory
from src.config.logger import setup_logger
from src.exception.custom_exception import QAError

logger = setup_logger(__name__)

# Initialize conversational memory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# HuggingFace LLM Model Name
LLM_MODEL_NAME = "google/flan-t5-small"  # lightweight; you can change to better ones later

def get_llm_pipeline():
    """
    Loads a HuggingFace text-generation pipeline.

    Returns:
        HuggingFacePipeline: A wrapped LLM model for LangChain.
    """
    try:
        hf_pipeline = pipeline(
            "text2text-generation",
            model=LLM_MODEL_NAME,
            max_length=512,
            temperature=0
        )
        llm = HuggingFacePipeline(pipeline=hf_pipeline)
        logger.info(f"HuggingFace LLM loaded: {LLM_MODEL_NAME}")
        return llm
    except Exception as e:
        logger.error(f"Failed to load HuggingFace LLM: {e}")
        raise QAError(str(e))

def ask_question(vector_store, question: str):
    """
    Answers a user's question using the provided vector store and HuggingFace LLM.

    Args:
        vector_store (Chroma): The vector database.
        question (str): The question to be answered.

    Returns:
        str: The answer generated by the model.
    """
    try:
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        qa = ConversationalRetrievalChain.from_llm(
            llm=get_llm_pipeline(),
            retriever=retriever,
            memory=memory,
        )
        result = qa.run(question)

        logger.info(f"Answered question: {question}")
        return result

    except Exception as e:
        logger.error(f"Question answering failed: {e}")
        raise QAError(str(e))
